---
title: "STA 465 Homework 4"
author: "Tianyi Zhang 1005156607"
date: "11/04/2022"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE, message=FALSE}
load("C:/Users/psymi/Desktop/Year 4/STA465/Homework 4/Hwk4Data.RData")
library(INLA)
library(tidyverse)
library(knitr)
library(rgeos)
library(tmap)
library(sp)
library(raster)

# for some reason Hwk4Data.RData doesn't contain ncol?
ncol <- ncol(r)
```

# Question 1: Sloths in Costa Rica

## Question 1.1

```{r Q1.1}
grid
```

Looking at the `grid` dataset, we can see that it is of the
SpatialPolygonsDataFrame class and its CRS is WGS84.

## Question 1.2

```{r Q1.2}
#### Model 1; Weakly Informative Priors ####
# default values for priors for an rw2d and iid model are 
# both log-gamma(1, 0.00005)
prior.weakly <- list(prec = list(prior = "normal",
                               param = c(1, 0.01)))

formula <- Y ~ 1 + cov +
  f(id, model="rw2d", nrow = nrow, ncol = ncol, hyper = prior.weakly) +
  f(id2, model="iid", hyper = prior.weakly)
res <- inla(formula, family = "poisson", data = grid@data,
    E = cellarea, control.predictor = list(compute = TRUE))

summary(res)

#Organize Results in a Table
cred.int <- data.frame(LowerBound = res$summary.fixed$`0.025quant`,
                       UpperBound = res$summary.fixed$`0.975quant`,
                       Estimate = res$summary.fixed$mean)
rownames(cred.int)<- c("Intercept","Covariate")
cred.int %>%
  kable(
    caption = "95% Credible Intervals For Parameter Estimates,
      Weakly Informative Prior",
    col.names = c("Lower Bound", "Upper Bound", "Estimate"),
    row.names = TRUE,
    digits = 4,
    booktabs = TRUE
  )

#### Model 2; Non-Informative Priors ####
prior.non <- list(prec = list(prior = "loggamma",
                               param = c(1, 0.00001)))

formula <- Y ~ 1 + cov +
  f(id, model="rw2d", nrow = nrow, ncol = ncol, hyper = prior.non) +
  f(id2, model="iid", hyper = prior.non)
res2 <- inla(formula, family = "poisson", data = grid@data,
    E = cellarea, control.predictor = list(compute = TRUE))

summary(res2)

#Organize Results in a Table
cred.int2 <- data.frame(LowerBound = res2$summary.fixed$`0.025quant`,
                       UpperBound = res2$summary.fixed$`0.975quant`,
                       Estimate = res2$summary.fixed$mean)
rownames(cred.int2)<- c("Intercept","Covariate")
cred.int2 %>%
  kable(
    caption = "95% Credible Intervals For Parameter Estimates,
      Non-Informative Prior",
    col.names = c("Lower Bound", "Upper Bound", "Estimate"),
    row.names = TRUE,
    digits = 4,
    booktabs = TRUE
  )
```

The tables show that the estimates and the 95% credible intervals for covariates
in the weakly and non-informative priors are very similar. The
95% credible intervals and the estimates for the intercept in the model with
weakly-informative prior is slightly lower in general.

## Quesiton 1.3

### Weakly Informative Model

The following maps are for the weakly informative model.

```{r Q1.3a, warning=FALSE, message=FALSE}
#### Create maps of the random effects ####
# Weakly informative model
grid$respa <- res$summary.random$id[grid$id, "mean"]
grid$reiid <- res$summary.random$id2[, "mean"]

tm_shape(grid) +
  tm_polygons(col = c("respa", "reiid"), style = "cont", border.col = "transparent")  +
  tm_shape(gridborder) + tm_borders() +
  tm_facets(ncol = 2) + tm_legend(legend.position = c("left", "bottom"))

# Predicted counts per cell
cellarea <- resolution*resolution
grid$NE <- res$summary.fitted.values[, "mean"] * cellarea
grid$LL <- res$summary.fitted.values[, "0.025quant"] * cellarea
grid$UL <- res$summary.fitted.values[, "0.975quant"] * cellarea
#summary(grid)

# Create maps for the predicted counts, its lower interval, and its upper
# interval respectively
# NE: map of predicted counts
# LL: map of lower limit
# UL: map of upper limit
tm_shape(grid) +
  tm_polygons(col = c("NE", "LL", "UL"),
              style = 'fixed', border.col = "transparent",
              breaks = c(0, 10, 50, 100, ceiling(max(grid$UL)))) +
  tm_shape(gridborder) + tm_borders() +
  tm_facets(ncol = 3) + tm_legend(legend.position = c("left", "bottom")) 
```

The first two maps are maps of the random effects, rw2d and iid, respectively.

The next set of three maps are maps of the predicted counts per cell, its 95%
lower limit and its upper limit, respectively.

### Non-Informative Model

The following maps are for the non-informative model.

```{r Q1.3b, warning=FALSE, message=FALSE}
#### Create maps of the random effects ####
# Non-informative model
grid$respa <- res2$summary.random$id[grid$id, "mean"]
grid$reiid <- res2$summary.random$id2[, "mean"]

tm_shape(grid) +
  tm_polygons(col = c("respa", "reiid"), style = "cont", border.col = "transparent")  +
  tm_shape(gridborder) + tm_borders() +
  tm_facets(ncol = 2) + tm_legend(legend.position = c("left", "bottom"))

# Predicted counts per cell
cellarea <- resolution*resolution
grid$NE <- res2$summary.fitted.values[, "mean"] * cellarea
grid$LL <- res2$summary.fitted.values[, "0.025quant"] * cellarea
grid$UL <- res2$summary.fitted.values[, "0.975quant"] * cellarea
#summary(grid)

# Create maps for the predicted counts, its lower interval, and its upper
# interval respectively
# NE: map of predicted counts
# LL: map of lower limit
# UL: map of upper limit
tm_shape(grid) +
  tm_polygons(col = c("NE", "LL", "UL"),
              style = 'fixed', border.col = "transparent",
              breaks = c(0, 10, 50, 100, ceiling(max(grid$UL)))) +
  tm_shape(gridborder) + tm_borders() +
  tm_facets(ncol = 3) + tm_legend(legend.position = c("left", "bottom")) 
```

The first two maps are maps of the random effects, rw2d and iid, respectively.

The next set of three maps are maps of the predicted counts per cell, its 95%
lower limit and its upper limit, respectively.

There does not seem to be any major differences in the predicted cell counts and
its 95% credible intervals for the weakly informative prior and non-informative
prior.

# Question 2

## Question 2.1

```{r Q2.1, warning=FALSE, message=FALSE}
#### Get data ####
# Data acquisition code is from Lab 5
library(SpatialEpi)
library(sf)
library(spdep)

data(pennLC)
population <- pennLC$data$population
cases <- pennLC$data$cases
n.strata <- 16
E <- expected(population, cases, n.strata)
d <- aggregate(x = pennLC$data$cases, by = list(county = pennLC$data$county), FUN = sum)

# convert from spatial polygon to simple feature
pennLC.sf <- st_as_sf(pennLC$spatial.polygon)
pennLC.sf$county <- d$county
pennLC.sf$counts <- d$x
pennLC.sf$E <- E[match(pennLC.sf$county, unique(pennLC$data$county))]
pennLC.sf <- merge(pennLC.sf, pennLC$smoking, by.x = "county", by.y = "county")
pennLC.sf <- pennLC.sf%>%
  mutate(SIR = counts/E)

#### Fit models ####
# Using Default Priors
##### Complete pooling and smoking covariate (no random effect) #####
formula.2.1.1 <- counts ~ 1 + smoking
res.2.1.1 <- inla(formula.2.1.1, family = "poisson", data = pennLC.sf,
            E = pennLC.sf$E, control.predictor = list(compute = TRUE),
            control.compute = list(cpo=TRUE))

##### Hierarchical random effect (iid) - (intercept only) #####
formula.2.1.2 <- counts ~ 1 + f(county, model = "iid")
res.2.1.2 <- inla(formula.2.1.2, family = "poisson", data = pennLC.sf,
            E = pennLC.sf$E, control.predictor = list(compute = TRUE),
            control.compute = list(cpo=TRUE))

##### Hierarchical random effect (iid) + smoking covariate #####
# add a column of numbers (need both intercept and slope to vary)
pennLC.sf$county_dup <- pennLC.sf$county
formula.2.1.3 <- counts ~ 1 + smoking + f(county, model = "iid") +
  f(county_dup, smoking, model = "iid")
res.2.1.3 <- inla(formula.2.1.3, family = "poisson", data = pennLC.sf,
            E = pennLC.sf$E, control.predictor = list(compute = TRUE),
            control.compute = list(cpo=TRUE))

##### Spatial + iid random effect #####
# use BYM for spatial random effect
## Values of E_i and neighborhood structure
E.penn <- pennLC.sf$E
neighbor.penn <- poly2nb(pennLC.sf)

nb2INLA("npenn.adj", neighbor.penn)
g <- inla.read.graph(filename = "npenn.adj")

pennLC.sf$re_u <- 1:nrow(pennLC.sf)
pennLC.sf$re_v <- 1:nrow(pennLC.sf)

formula.2.1.4 <- counts ~ 1 + f(re_u, model = "besag", graph = g) +
  f(re_v, model = "iid")
res.2.1.4 <- inla(formula.2.1.4, family = "poisson", data = pennLC.sf,
            E = pennLC.sf$E, control.predictor = list(compute = TRUE),
            control.compute = list(cpo=TRUE))

##### Spatial + iid random effect + smoking covariate #####
formula.2.1.5 <- counts ~ smoking + f(re_u, model = "besag", graph = g) +
  f(re_v, model = "iid")
res.2.1.5 <- inla(formula.2.1.5, family = "poisson", data = pennLC.sf,
            E = pennLC.sf$E, control.predictor = list(compute = TRUE),
            control.compute = list(cpo=TRUE))
```

### 2.1.1: Complete pooling and smoking covariate (no random effect)

```{r Q2.1.1, warning=FALSE, message=FALSE}
#### Make a table of CPO and PIT values ####
cpo.2.1.1 <- data.frame(CPO = res.2.1.1$cpo$cpo,
                       PIT = res.2.1.1$cpo$pit)
cpo.2.1.1 %>%
  kable(
    caption = "CPO and PIT values for Complete Pooling and Smoking 
    Covariate (no random effect)",
    row.names = FALSE,
    digits = 4,
    booktabs = TRUE,
    align = 'l'
  )

#### Create maps of predicted prevalence and its standard deviation ####
pennLC.sf$m2.1.1.mean <- res.2.1.1$summary.fitted.values[,"mean"]*E.penn
pennLC.sf$m2.1.1.sd <- res.2.1.1$summary.fitted.values[,"sd"]*E.penn

res.2.1.1.mean <- tm_shape(pennLC.sf) + 
  tm_polygons("m2.1.1.mean") + 
  tm_layout(main.title = "Predicted Prevalence")

res.2.1.1.sd <- tm_shape(pennLC.sf) + 
  tm_polygons("m2.1.1.sd") + 
  tm_layout(main.title = "Standard Deviation of Predicted Prevalence")

tmap_arrange(res.2.1.1.mean, res.2.1.1.sd, ncol=1)
```

### 2.1.2: Hierarchical random effect (iid) - (intercept only)

```{r Q2.1.2, warning=FALSE, message=FALSE}
#### Make a table of CPO and PIT values ####
cpo.2.1.2 <- data.frame(CPO = res.2.1.2$cpo$cpo,
                       PIT = res.2.1.2$cpo$pit)
cpo.2.1.2 %>%
  kable(
    caption = "CPO and PIT values for Hierarchical random effect (iid)
    - (intercept only)",
    row.names = FALSE,
    digits = 4,
    booktabs = TRUE,
    align = 'l'
  )

#### Create maps of predicted prevalence and its standard deviation ####
pennLC.sf$m2.1.2.mean <- res.2.1.2$summary.fitted.values[,"mean"]*E.penn
pennLC.sf$m2.1.2.sd <- res.2.1.2$summary.fitted.values[,"sd"]*E.penn

res.2.1.2.mean <- tm_shape(pennLC.sf) + 
  tm_polygons("m2.1.2.mean") + 
  tm_layout(main.title = "Predicted Prevalence")

res.2.1.2.sd <- tm_shape(pennLC.sf) + 
  tm_polygons("m2.1.2.sd") + 
  tm_layout(main.title = "Standard Deviation of Predicted Prevalence")

tmap_arrange(res.2.1.2.mean, res.2.1.2.sd, ncol=1)
```

### 2.1.3: Hierarchical random effect (iid) + smoking covariate

```{r Q2.1.3, warning=FALSE, message=FALSE}
#### Make a table of CPO and PIT values ####
cpo.2.1.3 <- data.frame(CPO = res.2.1.3$cpo$cpo,
                       PIT = res.2.1.3$cpo$pit)
cpo.2.1.3 %>%
  kable(
    caption = "CPO and PIT values for Hierarchical random effect (iid)
    + smoking covariate",
    row.names = FALSE,
    digits = 4,
    booktabs = TRUE,
    align = 'l'
  )

#### Create maps of predicted prevalence and its standard deviation ####
pennLC.sf$m2.1.3.mean <- res.2.1.3$summary.fitted.values[,"mean"]*E.penn
pennLC.sf$m2.1.3.sd <- res.2.1.3$summary.fitted.values[,"sd"]*E.penn

res.2.1.3.mean <- tm_shape(pennLC.sf) + 
  tm_polygons("m2.1.3.mean") + 
  tm_layout(main.title = "Predicted Prevalence")

res.2.1.3.sd <- tm_shape(pennLC.sf) + 
  tm_polygons("m2.1.3.sd") + 
  tm_layout(main.title = "Standard Deviation of Predicted Prevalence")

tmap_arrange(res.2.1.3.mean, res.2.1.3.sd, ncol=1)
```

### 2.1.4: Spatial + iid random effect

```{r Q2.1.4, warning=FALSE, message=FALSE}
#### Make a table of CPO and PIT values ####
cpo.2.1.4 <- data.frame(CPO = res.2.1.4$cpo$cpo,
                       PIT = res.2.1.4$cpo$pit)
cpo.2.1.4 %>%
  kable(
    caption = "CPO and PIT values for Spatial + iid random effect",
    row.names = FALSE,
    digits = 4,
    booktabs = TRUE,
    align = 'l'
  )

#### Create maps of predicted prevalence and its standard deviation ####
pennLC.sf$m2.1.4.mean <- res.2.1.4$summary.fitted.values[,"mean"]*E.penn
pennLC.sf$m2.1.4.sd <- res.2.1.4$summary.fitted.values[,"sd"]*E.penn

res.2.1.4.mean <- tm_shape(pennLC.sf) + 
  tm_polygons("m2.1.4.mean") + 
  tm_layout(main.title = "Predicted Prevalence")

res.2.1.4.sd <- tm_shape(pennLC.sf) + 
  tm_polygons("m2.1.4.sd") + 
  tm_layout(main.title = "Standard Deviation of Predicted Prevalence")

tmap_arrange(res.2.1.4.mean, res.2.1.4.sd, ncol=1)
```

### 2.1.5: Spatial + iid random effect + smoking covariate

```{r Q2.1.5, warning=FALSE, message=FALSE}
#### Make a table of CPO and PIT values ####
cpo.2.1.5 <- data.frame(CPO = res.2.1.5$cpo$cpo,
                       PIT = res.2.1.5$cpo$pit)
cpo.2.1.5 %>%
  kable(
    caption = "CPO and PIT values for 
    Spatial + iid random effect + smoking covariate",
    row.names = FALSE,
    digits = 4,
    booktabs = TRUE,
    align = 'l'
  )

#### Create maps of predicted prevalence and its standard deviation ####
pennLC.sf$m2.1.5.mean <- res.2.1.5$summary.fitted.values[,"mean"]*E.penn
pennLC.sf$m2.1.5.sd <- res.2.1.5$summary.fitted.values[,"sd"]*E.penn

res.2.1.5.mean <- tm_shape(pennLC.sf) + 
  tm_polygons("m2.1.5.mean") + 
  tm_layout(main.title = "Predicted Prevalence")

res.2.1.5.sd <- tm_shape(pennLC.sf) + 
  tm_polygons("m2.1.5.sd") + 
  tm_layout(main.title = "Standard Deviation of Predicted Prevalence")

tmap_arrange(res.2.1.5.mean, res.2.1.5.sd, ncol=1)
```

There does not seem to be any major difference between the predicted
prevalence between all five models; The two counties with the highest
prevalence of lung cancer always seems to be Allegheny and Philadelphia.

In the complete pooling model (2.1.1) and the spatial + iid random effect
+ smoking covariate model (2.1.5), the predicted prevalence is relatively
lower when compared to the other models.

In the spatial + iid random effect model (2.1.4), the estimated prevalence in
Montgomery is relatively higher when compared to the other models. 

## Question 2.2

```{r Q2.2a, warning=FALSE, message=FALSE}
#### Organize Results in a Table: Estimates and Credible Intervals ####
##### 2.2.1: Complete pooling and smoking covariate (no random effects) #####
table.2.2.1 <- data.frame(LowerLimit = res.2.1.1$summary.fixed$`0.025quant`,
                       UpperLimit = res.2.1.1$summary.fixed$`0.975quant`,
                       Estimate = res.2.1.1$summary.fixed$mean)
rownames(table.2.2.1)<- c("Model 1 Intercept","Model 1 Covariate")

##### 2.2.2: Hierarchical random effect (iid) - (intercept only) #####
table.2.2.2 <- data.frame(LowerLimit = res.2.1.2$summary.fixed$`0.025quant`,
                       UpperLimit = res.2.1.2$summary.fixed$`0.975quant`,
                       Estimate = res.2.1.2$summary.fixed$mean)
rownames(table.2.2.2)<- c("Model 2 Intercept")

##### 2.2.3: Hierarchical random effect (iid) + smoking covariate #####
table.2.2.3 <- data.frame(LowerLimit = res.2.1.3$summary.fixed$`0.025quant`,
                       UpperLimit = res.2.1.3$summary.fixed$`0.975quant`,
                       Estimate = res.2.1.3$summary.fixed$mean)
rownames(table.2.2.3)<- c("Model 3 Intercept","Model 3 Covariate")

##### 2.2.4: Spatial + iid random effect #####
table.2.2.4 <- data.frame(LowerLimit = res.2.1.4$summary.fixed$`0.025quant`,
                       UpperLimit = res.2.1.4$summary.fixed$`0.975quant`,
                       Estimate = res.2.1.4$summary.fixed$mean)
rownames(table.2.2.4)<- c("Model 4 Intercept")

##### 2.2.5: Spatial + iid random effect #####
table.2.2.5 <- data.frame(LowerLimit = res.2.1.5$summary.fixed$`0.025quant`,
                       UpperLimit = res.2.1.5$summary.fixed$`0.975quant`,
                       Estimate = res.2.1.5$summary.fixed$mean)
rownames(table.2.2.5)<- c("Model 5 Intercept","Model 5 Covariate")

##### Merge all tables #####
table.2.2 <- bind_rows(table.2.2.1, table.2.2.2, table.2.2.3, table.2.2.4,
                       table.2.2.5)
table.2.2 %>%
  kable(
    caption = "Parameter Estimates and 95% Credible Intervals For All 5 Models",
    col.names = c("Lower Bound", "Upper Bound", "Estimate"),
    row.names = TRUE,
    digits = 4,
    booktabs = TRUE
  )

#### Organize Results in a Table: Sum of Log(CPO) ####
sum_cpo1 <- sum(log(res.2.1.1$cpo$cpo))
sum_cpo2 <- sum(log(res.2.1.2$cpo$cpo))
sum_cpo3 <- sum(log(res.2.1.3$cpo$cpo))
sum_cpo4 <- sum(log(res.2.1.4$cpo$cpo))
sum_cpo5 <- sum(log(res.2.1.5$cpo$cpo))

sum_cpo <- t(data.frame(sum_cpo1, sum_cpo2, sum_cpo3, sum_cpo4, sum_cpo5))
rownames(sum_cpo) <- c("Model 1", "Model 2", "Model 3", "Model 4", "Model 5")

sum_cpo %>%
  kable(
    caption = "Sum of Log(CPO) for All 5 Models",
    col.names = "Sum of Log(CPO)",
    row.names = TRUE,
  )
```

A smaller value of $-\sum log(CPO)$ points to a better model fit. Therefore,
Model 5, Spatial + iid random effect + smoking covariate, is the model with the
best fit.

```{r Q2.2b, warning=FALSE, message=FALSE}
#### Plot Histogram of the PIT values ####
##### Model 1: Complete pooling and smoking covariate (no random effects) #####
pit1 <- data.frame(PIT = res.2.1.1$cpo$pit)

ggplot(data = pit1, aes(x=PIT)) + 
  geom_histogram(binwidth=0.05, color="black", fill="lightblue") +
  theme_classic()

##### Model 2: Hierarchical random effect (iid) - (intercept only) #####
pit2 <- data.frame(PIT = res.2.1.2$cpo$pit)

ggplot(data = pit2, aes(x=PIT)) + 
  geom_histogram(binwidth=0.05, color="black", fill="lightblue") +
  labs(title="PIT Histogram for Model 2") +
  theme_classic()

##### Model 3: Hierarchical random effect (iid) + smoking covariate #####
pit3 <- data.frame(PIT = res.2.1.3$cpo$pit)

ggplot(data = pit3, aes(x=PIT)) + 
  geom_histogram(binwidth=0.05, color="black", fill="lightblue") +
  labs(title="PIT Histogram for Model 3") +
  theme_classic()

##### Model 4: Spatial + iid random effect #####
pit4 <- data.frame(PIT = res.2.1.4$cpo$pit)

ggplot(data = pit4, aes(x=PIT)) + 
  geom_histogram(binwidth=0.05, color="black", fill="lightblue") +
  labs(title="PIT Histogram for Model 4") +
  theme_classic()

##### Model 4: Spatial + iid random effect + smoking covariate #####
pit5 <- data.frame(PIT = res.2.1.5$cpo$pit)

ggplot(data = pit5, aes(x=PIT)) + 
  geom_histogram(binwidth=0.05, color="black", fill="lightblue") +
  labs(title="PIT Histogram for Model 5") +
  theme_classic()
```

In general, if the model represents the observations well, we should see a
uniform distribution in the histogram of PIT values. By looking at the PIT
histograms for each model, we see that Model 4 and Model 5 have a relatively
uniform distribution when compared to the other models. Therefore, the 
histograms of PIT values tell us that Model 4 and Model 5 represent the
observations the best.

# Question 3

```{r Q3a, warning=FALSE, message=FALSE}
#### Setup ####
## load in the R packages needed and data
library(geoR)
library(rgdal)
library(geodist)

data(gambia)

## aggregate by location and convert the object into a 'sf' object
d <- group_by(gambia, x, y) %>%
  summarize(
    total = n(),
    positive = sum(pos),
    prev = positive / total
  )

sps <- SpatialPoints(d[, c("x", "y")],
  proj4string = CRS("+proj=utm +zone=28")
)
spst <- spTransform(sps, CRS("+proj=longlat +datum=WGS84"))
d[,c("long", "lat")] <- spst@coords
r <- getData(name = "alt", country = "GMB", mask = TRUE)
d$alt <- raster::extract(r, d[, c("long", "lat")])

## Find and fill the missing altitudes
r_sen <- getData(name = "alt", country = "SEN", mask = TRUE)

sen_alt <- raster::extract(r_sen, d[, c("long", "lat")])

d$alt[25] <- 31
d$alt[64] <- 20

gambia.sf <- st_as_sf(d, coords = c("long", "lat"), crs = "+proj=longlat +datum=WGS84")

## map of the prevalence data
tmap_mode("view")
tm_shape(gambia.sf) + tm_dots("prev", title = "Prevalence")
```

From the map, we can see that the prevalence data is generally split into four
secitons based on their geographical location. The points are concentrated near
one of four regions: Kerewan, Brikama, JanJan Bureh, and Basse. Therefore,
we will partition the data into 4 folds based on these regions.

```{r Q3b, warning=FALSE, message=FALSE}
#### Perform 4-fold CV ####
## Model: Spatial + iid random effect + altitude covariate
gambia.sf$id <- 1:nrow(gambia.sf)
gambia.sf$id2 <- 1:nrow(gambia.sf)

formula <- prev ~ 1 + alt +
  f(id, model = "rw2d", nrow = nrow, ncol = ncol) +
  f(id2, model = "iid")

##### Parition the points based on Coordinates #####
# partition for Kerewan
part1 <- filter(gambia.sf, x > 363790 & x < 397080 & y > 1490500 & y < 1502000)
# partition for Brikama
part2 <- filter(gambia.sf, x > 349600 & x < 390500 & y > 1458000 & y < 1466500)
# partition for JanJan Bureh
part3 <- filter(gambia.sf, x > 486500 & x < 524000 & y > 1485000 & y < 1515000)
# partition for Basse
part4 <- filter(gambia.sf, x > 572000 & x < 622500 & y > 1467000 & y < 1500000)

tm_shape(part1) + tm_dots("prev", col = "red") +
  tm_layout(title = "Kerewan Partition")
tm_shape(part2) + tm_dots("prev", col = "lightblue") +
  tm_layout(title = "Brikama Partition")
tm_shape(part3) + tm_dots("prev", col = "green") +
  tm_layout(title = "JanJan Bureh Partition")
tm_shape(part4) + tm_dots("prev", col = "yellow") +
  tm_layout(title = "Basse Partition")
```

The four maps above showcase the geographically partitioned folds. A 4-fold
CV is performed and the mean squared error (MSE) in each process is shown in
the table below:

```{r Q3c, warning=FALSE, message=FALSE}
#### Repeat process four times ####
##### First time #####
# To predict points in Kerewan region, set the prev of points in the said region
# to NA
N <- nrow(part1)
pred1 <- gambia.sf %>%
  filter(x > 363790 & x < 397080 & y > 1490500 & y < 1502000) %>%
  mutate(prev = NA) %>%
  bind_rows(part2,part3,part4)
  
res.3.1 <- inla(formula, family = "gaussian", data = pred1,
                control.predictor = list(compute = TRUE))

pred_mean.3.1 <- res.3.1$summary.fitted.values[1:N, "mean"]
# Calculate MSE
MSE.3.1 <- sum((part1$prev - pred_mean.3.1)^2)/N

##### Second time #####
N <- nrow(part2)
pred2 <- gambia.sf %>%
  filter(x > 349600 & x < 390500 & y > 1458000 & y < 1466500) %>%
  mutate(prev = NA) %>%
  bind_rows(part1,part3,part4)
  
res.3.2 <- inla(formula, family = "gaussian", data = pred2,
                control.predictor = list(compute = TRUE))

pred_mean.3.2 <- res.3.2$summary.fitted.values[1:N, "mean"]
# Calculate MSE
MSE.3.2 <- sum((part2$prev - pred_mean.3.2)^2)/N

##### Third time #####
N <- nrow(part3)
pred3 <- gambia.sf %>%
  filter(x > 486500 & x < 524000 & y > 1485000 & y < 1515000) %>%
  mutate(prev = NA) %>%
  bind_rows(part1,part2,part4)
  
res.3.3 <- inla(formula, family = "gaussian", data = pred3,
                control.predictor = list(compute = TRUE))

pred_mean.3.3 <- res.3.3$summary.fitted.values[1:N, "mean"]
# Calculate MSE
MSE.3.3 <- sum((part3$prev - pred_mean.3.3)^2)/N

##### Fourth time #####
N <- nrow(part4)
pred4 <- gambia.sf %>%
  filter(x > 572000 & x < 622500 & y > 1467000 & y < 1500000) %>%
  mutate(prev = NA) %>%
  bind_rows(part1,part2,part3)
  
res.3.4 <- inla(formula, family = "gaussian", data = pred4,
                control.predictor = list(compute = TRUE))

pred_mean.3.4 <- res.3.4$summary.fitted.values[1:N, "mean"]
# Calculate MSE
MSE.3.4 <- sum((part4$prev - pred_mean.3.4)^2)/N

#### Create a table for the MSE ####
MSEs <- data.frame(MSE1 = MSE.3.1, MSE2 = MSE.3.2,
                   MSE3 = MSE.3.3, MSE4 = MSE.3.4)
MSEs %>%
  kable(
    caption = "Mean Squared Error in each repetition for 4-fold CV",
    col.names = c("MSE-Kere","MSE-Brik","MSE-JanJan","MSE-Basse"),
    row.names = FALSE,
    digits = 4,
    booktabs = TRUE
  )
```

\newpage

# References

- GÃ³mez-Rubio, V. (2021, August 29). Bayesian inference with INLA. Chapter 5 Priors in R-INLA. Retrieved April 12, 2022, from https://becarioprecario.bitbucket.io/inla-gitbook/ch-priors.html 


- Moraga, P. (n.d.). Geospatial Health Data: Modeling and visualization with R-INLA and shiny. Chapter 4 The R-INLA package. Retrieved April 12, 2022, from https://www.paulamoraga.com/book-geospatial/sec-inla.html?q=cross#model 

- Moraga, P. (n.d.). Species Distribution Modeling using Spatial Point Processes: a Case Study of Sloths in Costa Rica. Species distribution modeling using Spatial Point Processes: A case study of sloths in Costa Rica. Retrieved April 12, 2022, from https://www.paulamoraga.com/tutorial-point-patterns/#54_Fitting_the_model_using_INLA 

- Rue, H., Lindgren, F., Niekerk, J. van, Krainski, E., &amp; Fattah, E. A. (n.d.). INLA Project - FAQ. R-INLA Project. Retrieved April 12, 2022, from https://www.r-inla.org/faq#h.821k2r53fvx3 

- Asidianya, N, (2022, February 16) Lab Lecture #5. Quercus. https://q.utoronto.ca/

- Barajas, V. Leos, (2022, April 1) Lecture11 [PDF]. Quercus. https://q.utoronto.ca/